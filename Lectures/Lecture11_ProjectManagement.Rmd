---
title: "Lecture 11: Project Management"
author: "Kelly"
date: ''
output:
  html_document: default
  pdf_document: default
---

```{r, include = F}
library(knitr)
options(width=60)
require(knitr)
# Set so that long lines in R will be wrapped:
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=TRUE, message = F, warning = F, error = T, echo = T)
library(tidyverse)
library(lubridate)
library(yarrr)
```

I have always found it difficult to stay organized, particularly when a project doesn't have a clear trajectory: perhaps it starts as an email, or a stray thought, and then compounds into a thing having data and analysis and plots, etc.  

The larger context here is reproducible research. And here, your most important collaborator is your future self: you want to make sure that -- 6 or 12 or 18 months from now -- you could complete the same research and arrive at precisely the same answers as you do today.  See http://ropensci.github.io/reproducibility-guide/sections/introduction/


And, pulled from that link, a useful thought: ``*An article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code and data, that produced the result.*'' (Claerbout and Karrenbach 1992).  In our context: your final paper or problem set (or whatever) isn't itself scholarship.  The process by which you get to that end product is the scholarship, and you need to be able to point to what you did and why you did it. 


# Tips 

Given that you're going to be using **R**, it makes sense to have a cheap/easy way of setting up a structure for ideas and files for each project.  And then every project will have the same structure.  This will save you a ton of time in the long run, because it will be harder to lose things.  

A really nice overview of useful concepts: https://swcarpentry.github.io/r-novice-gapminder/02-project-intro/

Tips from that exercise:

>  * Treat data as read-only

> This is probably the most important goal of setting up a project. Data is typically time consuming and/or expensive to collect. Working with them interactively (e.g., in Excel) where they can be modified means you are never sure of where the data came from, or how it has been modified since collection. It is therefore a good idea to treat your data as “read-only”.

>  * Data Cleaning

> In many cases your data will be “dirty”: it will need significant preprocessing to get into a format R (or any other programming language) will find useful. This task is sometimes called “data munging”. I find it useful to store these scripts in a separate folder, and create a second “read-only” data folder to hold the “cleaned” data sets.

>  * Treat generated output as disposable

> Anything generated by your scripts should be treated as disposable: it should all be able to be regenerated from your scripts.


# File Structure and Storage

In brief, make a mental switch to create a new project folder any time you THINK you might work on something for more than a day.  It's free to make a project. There's no reason not to do it. 

one way to do this:

# Rstudio `Project`

Rstudio has a nice, built-in way of keeping things organized.  When you start a new project, start a new `Project`.  See https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects

> 1. Creates a project file (with an .Rproj extension) within the project directory. This file contains various project options (discussed below) and can also be used as a shortcut for opening the project directly from the filesystem.
  2. Creates a hidden directory (named .Rproj.user) where project-specific temporary files (e.g. auto-saved source documents, window-state, etc.) are stored.
  3. Loads the project into RStudio and display its name in the Projects toolbar (which is located on the far right side of the main toolbar)

When you open a project (by clicking on the .Rproj file, for example), Rstudio automatically shows you what files are associated w the project folder, etc.  Quite handy.


# Project Management Templates and Packages

Jimmy O'Donnell (former SMEA and NOAA postdoc) created this: https://github.com/jimmyodonnell/NewProject

You can download it, and then copy/paste the folder each time you start a new project.  Pretty useful for science-like things.

In this particular case, your new project folder will come with a set number of folders inside it:

  * Analysis.  Within this you may choose to have folders for scripts and output, or whatever else is handy.
  * Documents.  Often PDFs or other things relevant to the work you're doing in the project.
  * Data.  Primary data, treated as read-only.  
  * Figures. Any figures you generate.
  
... and it will also have a readme file in the main project folder, which you will edit to tell your future self (or others) what you're doing with the project. 

There are other ways of organizing files, of course, and you should pick one that works for you. But pick one! 




  * Another example is here: 
Building a Research Compendium...

https://github.com/ropensci/rrrpkg

A research compendium might just be a running text file that tells future-you (and others) what you did and why.  BUT let's be honest: you're never going to keep that up for long.  So you want structural techniques for forcing accountability and organization. 


# Plotting to PDF (or JPG, etc)

One important part of staying organized is generating plots, analyses, etc, and then exporting them from **R** to be saved on your hard drive. Consistent w the advice above, it's not these outputs that fundamentally matter -- the valuable part is the code you used to generate the outputs.  

But that doesn't mean you want to re-generate plots all the time.  And plots you save along the way are nice to be able to go back to and illustrate how your analysis has changed over time.

Here, we use **R** to write a new figure to a PDF file in your working directory, which could then be used in a Word doc or elsewhere. 

```{r}
pdf("myplot.pdf", width = 5, height = 5)  #tell R you want to make a pdf, and (optionally) tell it what size you want the canvas to be
  plot(mpg~hp, data = mtcars)             # Then make the plot
dev.off()                                 #then tell R you are done writing the plot out to a file, so turn off the graphics device. 

#all done!


#Note this works w ggplot also:

library(tidyverse)

pdf("myplot_ggplot.pdf", width = 5, height = 5)  
  mtcars %>% 
    ggplot(aes(y = mpg,x = hp)) +
      geom_point()
dev.off()       

#or also:

 myplot <- mtcars %>% 
    ggplot(aes(y = mpg,x = hp)) +
      geom_point() 

 ggsave(filename = "myplot_ggplot_ggsave.pdf", 
             plot = myplot)  #note ggsave() uses the size of the current graphics device (i.e., the size of the plot on your screen when you save it)




```


# Saving Files Where You Want Them

Knowing just a tiny bit of how your operating system works can save you a lot of time, because you can efficiently tell **R** precisely where to put a file. 

The concept: using the file `path`.  This is the bit of info that tells the computer where something is on the hard drive or on a network.  You know all about this already.  For example: `Kelly_Lab/SMEA550_2018_R_course/Rcourse_Autumn2019/NewProjectExample`

The only new thing here is that you want to be able to point your computer to a place to keep files, so that you can remain organized.

  * *Absolute paths* always unambiguously tell the computer where something is, because they give the whole hierarchical file structure. For example:  `/Volumes/GoogleDrive/My Drive/Kelly_Lab/SMEA550_2018_R_course/Rcourse_Autumn2019/NewProjectExample`.  This can be useful, because it works from anywhere on your computer, no matter what working directory you're in at the time.  BUT it won't work on anyone else's computer, or on your next computer, or if you change the name of some folder in that path. So...

  * *Relative paths* are shorter, and are context-dependent.  They give a location as *relative* to the current working directory.  This is useful because you could move a whole project folder to someone else's computer, and it would still work.  But they can be dangerous because if you aren't sure what working directory you're in, you aren't sure where you're putting files.  For example:  

    + `Rcourse_Autumn2019/NewProjectExample` is a relative path that makes sense if I'm working in the folder `SMEA550_2019_R_course`. It is a folder that is further down the hierarchical structure (below) where I am currently working.  

    + `../Kelly_Lab` is a relative path that makes sense from the same working folder; it references the folder one hierarchical level ABOVE where I am currently working. 

So, let's say you have the following folder structure:

NewProjectExample 
 |__Data
 |__Analysis
 |__Figures
 
 and you are presently in the working directory `NewProjectExample`.  
 
```{r}
setwd("/Volumes/GoogleDrive/My Drive/RPKDesktop/SMEA/SMEA_550C_Rcourse_2019/NewProjectExample")

# Save a plot to the subfolder `Figures`
  pdf("Figures/myplot.pdf", width = 5, height = 5)  #note here, we specify the subfolder
    plot(mpg~hp, data = mtcars)
  dev.off()     

# Save a csv to the subfolder `Data` (one level BELOW your current working directory)
  write.csv(mtcars, "Data/mtcars1.csv")  #here again, specify the path

# Save a csv to the project folder itself (your current working directory)
  write.csv(mtcars, "mtcars2.csv")  #you're in the current wd, so you don't need a longer path
  
# Save a csv to the folder one hierarchical level ABOVE your current wd
  write.csv(mtcars, "../mtcars3.csv")  
  


```
 
Here you saw the problem with Absolute paths and `setwd()`. The previous chunk **only** works in Ryan's computer, or at least it needs to be a Mac with that very same path to `/NewProjectExample`. This is convenient if you run code for your own amusement, you don't plan to change any folder names in your computer, or you will never change computers. But if you:

  * 1. Want your code and research to be collaborative or reviewed,
  * 2. Work from two or more computers, or
  * 3. You are afraid of your future self changing directories' names ... 

then you need to do something to let your computer know (and every computer that uses your script) where is the right spot to start looking for files, i.e. where should all relative paths start from. Is like establishing a flag, somewhere in your hardrive, and make that flag transferrable between comnputers.

There are two options (or at least be know of two).

  * 1. Using **`Rprojects`**. In addition of all the wonderful things we said before about Rprojects, the folder in which your `.Rproj` file is present will be the working directory for that project. That way, all your scripts can read data with `read_csv("Data/my.raw.data.csv")`, write out modified files with `write_csv("Output/Stats.per.population.csv")` and figures with `pdf("Figures/myplot.pdf")` or `ggsave("Figures/barnacles.per.site.png")`
  
  * 2. Using the `here()` package
 
 

# here()

One interesting solution to the problem of specifying more durable paths is a packaged called `here()`

https://www.tidyverse.org/articles/2017/12/workflow-vs-script/


Some people have strong opinions about such things. For example:

![](/Volumes/GoogleDrive/My Drive/RPKDesktop/SMEA/SMEA_550C_Rcourse_2019/JennyBryan_Screenshot.png)



it works like this:

```{r}
library(here)  #load the library, and it automatically tells you what folder you're working in. 

#you can then easily define useful locations, like:

here("Analysis")

#or 

here("Analysis/Scripts/R")

#...and these paths would change automatically when you start your script from different working directories. 

```








